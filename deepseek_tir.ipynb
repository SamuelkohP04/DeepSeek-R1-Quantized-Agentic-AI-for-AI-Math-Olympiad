{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8099570,"sourceType":"datasetVersion","datasetId":4782935},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":12481604,"sourceType":"datasetVersion","datasetId":7865781},{"sourceId":171894765,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kohkaichunsamuel/deepseek-tir?scriptVersionId=293696484\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Setting Up VLLM Package to Run the LLM\n\nWe require to set up the v-LLM package that runs on Python 3.12.12.\n\nSource from: https://www.kaggle.com/code/haradibots/installing-vllm-offline-fix","metadata":{}},{"cell_type":"code","source":"# import os\n# !mkdir /kaggle/working/vllm_312_wheels\n# # This downloads exactly what Python 3.12 needs\n# !pip download vllm==0.9.2 -d /kaggle/working/vllm_312_wheels\n# # Zip it so you can download it easily\n# !zip -r vllm_312.zip /kaggle/working/vllm_312_wheels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:39.751846Z","iopub.execute_input":"2026-01-24T16:03:39.752144Z","iopub.status.idle":"2026-01-24T16:03:39.760829Z","shell.execute_reply.started":"2026-01-24T16:03:39.752111Z","shell.execute_reply":"2026-01-24T16:03:39.759853Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/input'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:39.763375Z","iopub.execute_input":"2026-01-24T16:03:39.766615Z","iopub.status.idle":"2026-01-24T16:03:39.793295Z","shell.execute_reply.started":"2026-01-24T16:03:39.766577Z","shell.execute_reply":"2026-01-24T16:03:39.790568Z"}},"outputs":[{"name":"stdout","text":"['vllm-0-9-2-offline-installer', 'aimo-vllm-accelerated-tot-sc-deepseekmath', 'deepseek-math', 'ai-mathematical-olympiad-progress-prize-3', 'vllm-whl', 'ai-mathematical-olympiad-prize', 'math-shepherd-mistral-7b-prm']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!find /kaggle/working -name \"vllm_312_wheels\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:39.794193Z","iopub.execute_input":"2026-01-24T16:03:39.794461Z","iopub.status.idle":"2026-01-24T16:03:39.964183Z","shell.execute_reply.started":"2026-01-24T16:03:39.794431Z","shell.execute_reply":"2026-01-24T16:03:39.963311Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/vllm_312_wheels\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Install directly from the folder shown in your screenshot\n!pip install --no-index --find-links=/kaggle/working/vllm_312_wheels vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:39.965628Z","iopub.execute_input":"2026-01-24T16:03:39.9663Z","iopub.status.idle":"2026-01-24T16:03:44.254993Z","shell.execute_reply.started":"2026-01-24T16:03:39.966256Z","shell.execute_reply":"2026-01-24T16:03:44.254281Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/working/vllm_312_wheels\nRequirement already satisfied: vllm in /usr/local/lib/python3.12/dist-packages (0.9.2)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2026.1.15)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm) (5.9.5)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm) (2.2.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm) (4.67.1)\nRequirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.53.3)\nRequirement already satisfied: huggingface-hub>=0.33.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[hf_xet]>=0.33.0->vllm) (0.36.0)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.21.4)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm) (5.29.5)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.123.10)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.3)\nRequirement already satisfied: openai<=1.90.0,>=1.52.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.90.0)\nRequirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.5)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\nRequirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (7.1.0)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\nRequirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.10.12)\nRequirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.7.30)\nRequirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.1.11)\nRequirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.2.2)\nRequirement already satisfied: xgrammar==0.1.19 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.1.19)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.3)\nRequirement already satisfied: partial-json-parser in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1.1.post7)\nRequirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\nRequirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from vllm) (0.20.0)\nRequirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.17.1)\nRequirement already satisfied: mistral_common>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from mistral_common[opencv]>=1.6.2->vllm) (1.8.8)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\nRequirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\nRequirement already satisfied: setuptools<80,>=77.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (79.0.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\nRequirement already satisfied: compressed-tensors==0.10.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.10.2)\nRequirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.18.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\nRequirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm) (1.1.1)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\nRequirement already satisfied: pybase64 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.4.3)\nRequirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.61.2)\nRequirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.53.0)\nRequirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.7.0)\nRequirement already satisfied: torchaudio==2.7.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.7.0)\nRequirement already satisfied: torchvision==0.22.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.0)\nRequirement already satisfied: xformers==0.0.30 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.0.30)\nRequirement already satisfied: astor in /usr/local/lib/python3.12/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.18.0->vllm) (0.4.0)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba==0.61.2->vllm) (0.44.0)\nRequirement already satisfied: interegular in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nRequirement already satisfied: diskcache in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\nRequirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (0.37.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (4.25.1)\nRequirement already satisfied: pycountry in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\nRequirement already satisfied: airportsdata in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (20250909)\nRequirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.12/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (3.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (2026.1.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (1.11.1.6)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->vllm) (3.3.0)\nRequirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.50.0)\nRequirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\nRequirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (26.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (1.2.0)\nRequirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.6.2->mistral_common[opencv]>=1.6.2->vllm) (2.11.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (4.12.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->vllm) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->vllm) (0.4.2)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.3.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.2)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2026.1.4)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.1->vllm) (0.7.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\nRequirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.1)\nRequirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.9.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.27.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\nRequirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.42.1)\nRequirement already satisfied: fastar>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip uninstall -y torch\n# # !pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:44.25621Z","iopub.execute_input":"2026-01-24T16:03:44.256916Z","iopub.status.idle":"2026-01-24T16:03:44.260029Z","shell.execute_reply.started":"2026-01-24T16:03:44.256869Z","shell.execute_reply":"2026-01-24T16:03:44.259482Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Testing the VLLM","metadata":{}},{"cell_type":"code","source":"import vllm\nprint(f\"vLLM version {vllm.__version__} successfully installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:44.260859Z","iopub.execute_input":"2026-01-24T16:03:44.261154Z","iopub.status.idle":"2026-01-24T16:03:46.478407Z","shell.execute_reply.started":"2026-01-24T16:03:44.26112Z","shell.execute_reply":"2026-01-24T16:03:46.477758Z"}},"outputs":[{"name":"stdout","text":"vLLM version 0.9.2 successfully installed!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:46.479427Z","iopub.execute_input":"2026-01-24T16:03:46.480048Z","iopub.status.idle":"2026-01-24T16:03:46.610027Z","shell.execute_reply.started":"2026-01-24T16:03:46.480022Z","shell.execute_reply":"2026-01-24T16:03:46.609198Z"}},"outputs":[{"name":"stdout","text":"Python 3.12.12\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:46.611294Z","iopub.execute_input":"2026-01-24T16:03:46.611528Z","iopub.status.idle":"2026-01-24T16:03:47.319829Z","shell.execute_reply.started":"2026-01-24T16:03:46.611501Z","shell.execute_reply":"2026-01-24T16:03:47.319068Z"}},"outputs":[{"name":"stdout","text":"4.53.3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# !pip install \"transformers<4.54.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:47.320747Z","iopub.execute_input":"2026-01-24T16:03:47.321154Z","iopub.status.idle":"2026-01-24T16:03:47.324439Z","shell.execute_reply.started":"2026-01-24T16:03:47.321126Z","shell.execute_reply":"2026-01-24T16:03:47.323715Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\nimport re\nimport sys\nimport subprocess\nfrom collections import defaultdict, Counter\nimport numpy as np\nfrom transformers import (AutoModelForCausalLM,\n    AutoTokenizer,\n    set_seed)\nimport torch\nimport math\n\nllm = LLM(model=\"/kaggle/input/deepseek-math\",\n          dtype='half',\n          enforce_eager=True,\n          gpu_memory_utilization=0.99,\n          swap_space=4,\n          max_model_len=2048,\n          kv_cache_dtype=\"fp8_e5m2\",\n          tensor_parallel_size=1)\n\nllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:03:47.325705Z","iopub.execute_input":"2026-01-24T16:03:47.326076Z","iopub.status.idle":"2026-01-24T16:04:36.387809Z","shell.execute_reply.started":"2026-01-24T16:03:47.326039Z","shell.execute_reply":"2026-01-24T16:04:36.387122Z"}},"outputs":[{"name":"stderr","text":"2026-01-24 16:03:49.345718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769270629.367392     610 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769270629.374187     610 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769270629.391672     610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769270629.391691     610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769270629.391693     610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769270629.391696     610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"INFO 01-24 16:03:54 [__init__.py:244] Automatically detected platform cuda.\nINFO 01-24 16:04:10 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\nWARNING 01-24 16:04:10 [config.py:3371] Casting torch.bfloat16 to torch.float16.\nINFO 01-24 16:04:10 [config.py:1472] Using max model len 2048\nWARNING 01-24 16:04:10 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nINFO 01-24 16:04:10 [config.py:1593] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\nWARNING 01-24 16:04:13 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 01-24 16:04:13 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/deepseek-math', speculative_config=None, tokenizer='/kaggle/input/deepseek-math', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/deepseek-math, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \nINFO 01-24 16:04:13 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-24 16:04:13 [cuda.py:360] Using XFormers backend.\nINFO 01-24 16:04:14 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 01-24 16:04:14 [model_runner.py:1171] Starting to load model /kaggle/input/deepseek-math...\n","output_type":"stream"},{"name":"stderr","text":"[W124 16:04:14.052079388 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W124 16:04:14.052784548 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8efb2c433b6c4069894a29d002b6bb64"}},"metadata":{}},{"name":"stdout","text":"INFO 01-24 16:04:27 [default_loader.py:272] Loading weights took 12.84 seconds\nINFO 01-24 16:04:28 [model_runner.py:1203] Model loading took 12.8726 GiB and 13.040472 seconds\nINFO 01-24 16:04:30 [worker.py:294] Memory profiling takes 1.73 seconds\nINFO 01-24 16:04:30 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.99) = 14.59GiB\nINFO 01-24 16:04:30 [worker.py:294] model weights take 12.87GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.95GiB; the rest of the memory reserved for KV Cache is 0.74GiB.\nINFO 01-24 16:04:31 [executor_base.py:113] # cuda blocks: 202, # CPU blocks: 1092\nINFO 01-24 16:04:31 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 1.58x\nINFO 01-24 16:04:36 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 7.70 seconds\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<vllm.entrypoints.llm.LLM at 0x79fee85c8cb0>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Instantiating the Tokeniser for the Process Reward Model (PRM)\n\nThe PRM was obtained from: https://www.kaggle.com/datasets/bsmit1659/math-shepherd-mistral-7b-prm","metadata":{}},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()\n\ngood_token = '+'\nbad_token = '-'\nstep_tag = 'ки'\n\nprm_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm')\n\nprm_tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:04:36.389058Z","iopub.execute_input":"2026-01-24T16:04:36.389369Z","iopub.status.idle":"2026-01-24T16:04:36.80187Z","shell.execute_reply.started":"2026-01-24T16:04:36.389343Z","shell.execute_reply":"2026-01-24T16:04:36.801034Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizerFast(name_or_path='/kaggle/input/math-shepherd-mistral-7b-prm', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Encoding the PRM","metadata":{}},{"cell_type":"code","source":"prm_candidate_tokens = prm_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\nstep_tag_id = prm_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n\nstep_tag_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:04:36.803073Z","iopub.execute_input":"2026-01-24T16:04:36.803433Z","iopub.status.idle":"2026-01-24T16:04:36.812395Z","shell.execute_reply.started":"2026-01-24T16:04:36.803394Z","shell.execute_reply":"2026-01-24T16:04:36.811628Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"12902"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### Initialising the PRM","metadata":{}},{"cell_type":"code","source":"prm_model = AutoModelForCausalLM.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm',\n                                                 torch_dtype=torch.float16,\n                                                 device_map=\"balanced_low_0\").eval()\nprm_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:04:36.816704Z","iopub.execute_input":"2026-01-24T16:04:36.817281Z","iopub.status.idle":"2026-01-24T16:06:35.158601Z","shell.execute_reply.started":"2026-01-24T16:04:36.817243Z","shell.execute_reply":"2026-01-24T16:06:35.15799Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0e2b9469d1747e49b92cd0897abfb25"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nPRIVATE = True\n\ndf = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.159491Z","iopub.execute_input":"2026-01-24T16:06:35.159809Z","iopub.status.idle":"2026-01-24T16:06:35.281619Z","shell.execute_reply.started":"2026-01-24T16:06:35.159779Z","shell.execute_reply":"2026-01-24T16:06:35.28098Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"       id                 problem\n0  000aaa          What is $1-1$?\n1  111bbb    What is $0\\times10$?\n2  222ccc  Solve $4+x=4$ for $x$.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>problem</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000aaa</td>\n      <td>What is $1-1$?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111bbb</td>\n      <td>What is $0\\times10$?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222ccc</td>\n      <td>Solve $4+x=4$ for $x$.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"if len(df) < 5:\n    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n    PRIVATE = False\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.282385Z","iopub.execute_input":"2026-01-24T16:06:35.282641Z","iopub.status.idle":"2026-01-24T16:06:35.302994Z","shell.execute_reply.started":"2026-01-24T16:06:35.282606Z","shell.execute_reply":"2026-01-24T16:06:35.302426Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       id                                            problem  answer\n0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n1  246d26  Each of the three-digits numbers $111$ to $999...     250\n2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n4  5277ed  There exists a unique increasing geometric seq...     211","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>problem</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>229ee8</td>\n      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>246d26</td>\n      <td>Each of the three-digits numbers $111$ to $999...</td>\n      <td>250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2fc4ad</td>\n      <td>Let the `sparkle' operation on positive intege...</td>\n      <td>702</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>430b63</td>\n      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n      <td>800</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5277ed</td>\n      <td>There exists a unique increasing geometric seq...</td>\n      <td>211</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Building the Naive Parser\n\nThis function only takes out the final answer (number) from the long answer the model generates","metadata":{}},{"cell_type":"code","source":"def naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.303701Z","iopub.execute_input":"2026-01-24T16:06:35.303913Z","iopub.status.idle":"2026-01-24T16:06:35.3081Z","shell.execute_reply.started":"2026-01-24T16:06:35.303893Z","shell.execute_reply":"2026-01-24T16:06:35.307469Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Top N Strings","metadata":{}},{"cell_type":"code","source":"import re\nimport sys\nimport os\nimport subprocess\n\ndef top_n_strings(input_list, n):\n    # Sort the list based on the prob values in descending order\n    sorted_list = sorted(input_list, key=lambda x: x[1], reverse=True)\n\n    # Get the top n elements\n    top_n = sorted_list[:n]\n\n    # Extract the strings from the top n elements\n    result = [item[0] for item in top_n]\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.308855Z","iopub.execute_input":"2026-01-24T16:06:35.309176Z","iopub.status.idle":"2026-01-24T16:06:35.322912Z","shell.execute_reply.started":"2026-01-24T16:06:35.30914Z","shell.execute_reply":"2026-01-24T16:06:35.322227Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Helper Functions for Phase 2 - Tool Integrated Reasoning (TIR)","metadata":{}},{"cell_type":"markdown","source":"### Process Output\n\nCuts the tiny code out and saves into a code.py block to run. If it fails, it gives an error.","metadata":{}},{"cell_type":"code","source":"def process_output(output):\n    result = output\n    \n    try:\n        code = output.split('```')[1][7:]\n\n        with open('code.py', 'w') as fout:\n            fout.write(code)\n\n        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n        try:\n            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n            print(shell_output)\n            code_output = round(float(eval(shell_output))) % 1000\n        except:\n            code_output = -1\n            \n        if os.path.exists('code.py'):\n            os.remove('code.py')\n\n        print('CODE RESULTS', code_output)\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING')\n        code_output = -1\n    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING')\n        result_output = -1\n    \n    return result_output, code_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.323839Z","iopub.execute_input":"2026-01-24T16:06:35.324232Z","iopub.status.idle":"2026-01-24T16:06:35.339897Z","shell.execute_reply.started":"2026-01-24T16:06:35.324196Z","shell.execute_reply":"2026-01-24T16:06:35.339212Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Function for Process Reward Model (PRM)\n\n- Takes all the different ideas the AI has, and gives each one a score (good or bad).\n  ","metadata":{}},{"cell_type":"code","source":"batch_size = 1\n\ndef eval_prm(candidates):\n    # Initialize a list to store all the log probabilities\n    all_log_probs = []\n\n    # Process the candidates in batches\n    for i in range(0, len(candidates), batch_size):\n        # Select a batch of candidates\n        batch_candidates = candidates[i:i + batch_size]\n\n        # Encode all candidates into a batch of input IDs\n        encoded_inputs = [prm_tokenizer.encode(candidate, return_tensors=\"pt\") for candidate in batch_candidates]\n\n        # Pad the encoded inputs to the same length\n        max_length = max([input_id.shape[1] for input_id in encoded_inputs])  # Find the longest sequence\n        padded_inputs = [\n            torch.nn.functional.pad(input_id, (0, max_length - input_id.size(1)), value=prm_tokenizer.pad_token_id) for\n            input_id in encoded_inputs]\n        input_ids = torch.cat(padded_inputs, dim=0).to(\"cuda:1\")  # Concatenate the padded inputs into a tensor\n\n        with torch.no_grad():\n            logits = prm_model(input_ids).logits[:, :, prm_candidate_tokens]\n\n            scores = logits.softmax(dim=-1)[:, :, 0].squeeze()\n\n            # Extract log probabilities for the specific candidate tokens\n            log_probs = scores.log()\n\n            if batch_size == 1:\n                batch_log_probs = log_probs[-1].flatten()\n            else:\n                batch_log_probs = log_probs[:, -1].flatten()\n\n            # Collect the log probabilities from this batch\n            all_log_probs.extend(batch_log_probs.cpu().tolist())\n\n    return all_log_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.340837Z","iopub.execute_input":"2026-01-24T16:06:35.341139Z","iopub.status.idle":"2026-01-24T16:06:35.359958Z","shell.execute_reply.started":"2026-01-24T16:06:35.341116Z","shell.execute_reply":"2026-01-24T16:06:35.35919Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Getting Stop Words","metadata":{}},{"cell_type":"code","source":"stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\nstop_words.append(\"\\n\")\n\ntool_stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\ntool_stop_words.append(\"```output\")\n\nprint(\"Stop words:\", stop_words)\nprint(\"Tool stop words:\", tool_stop_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.360784Z","iopub.execute_input":"2026-01-24T16:06:35.361046Z","iopub.status.idle":"2026-01-24T16:06:35.380591Z","shell.execute_reply.started":"2026-01-24T16:06:35.361024Z","shell.execute_reply":"2026-01-24T16:06:35.379977Z"}},"outputs":[{"name":"stdout","text":"Stop words: ['<｜end▁of▁sentence｜>', '\\n']\nTool stop words: ['<｜end▁of▁sentence｜>', '```output']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"sampling_params = SamplingParams(temperature=1.0,\n                                 max_tokens=256,\n                                 stop=stop_words)\n\ntool_sampling_params = SamplingParams(temperature=1.0,\n                                      max_tokens=2048,\n                                      stop=tool_stop_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.381447Z","iopub.execute_input":"2026-01-24T16:06:35.381743Z","iopub.status.idle":"2026-01-24T16:06:35.395882Z","shell.execute_reply.started":"2026-01-24T16:06:35.381712Z","shell.execute_reply":"2026-01-24T16:06:35.39516Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Prompts for DeepSeek to Solve","metadata":{}},{"cell_type":"code","source":"cot_instruction = \"\\nPlease reason step by step, and put your final \\\nanswer within \\\\boxed{}.\"\n\ntool_instruction = '\\nPlease integrate natural language reasoning \\\nwith programs to solve the problem above, and put your final answer \\\nwithin \\\\boxed{}.'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:06:35.396735Z","iopub.execute_input":"2026-01-24T16:06:35.397034Z","iopub.status.idle":"2026-01-24T16:06:35.41628Z","shell.execute_reply.started":"2026-01-24T16:06:35.397003Z","shell.execute_reply":"2026-01-24T16:06:35.415758Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### The Main Loop\n\n1. It lets the AI solve the problem multiple times, attaining about 48 different solutions.\n2. It uses the Judge to keep the best ones and eliminate the bad ones.\n3. If the AI is stuck, it will switch to Python coding using the Python interpreter. (TIR is used here).\n4. Finally, it counts all the answers it got and picks the one that appeared the most (majority voting).","metadata":{}},{"cell_type":"code","source":"n = 1\nall_prompts = []\ntotal_results = []\ntotal_answers = []\n\nfor i in tqdm(range(len(df))):\n    id_ = df['id'].loc[i]\n    problem = df['problem'].loc[i]\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": problem + cot_instruction\n        }\n    ]\n\n    base_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False\n    )\n    current_level = 0\n\n    current_level_nodes = [(base_prompt, 0)]  # Tuple of (node, cumulative_logprob)\n    completed_paths = []\n\n    while len(completed_paths) < n:\n        # Prepare batch for generation\n        batch_prompts = [node for node, _ in current_level_nodes]\n        batch_responses = llm.generate(batch_prompts*16, sampling_params)  # Generate for all nodes in a batch\n\n        prm_inputs = []  # To collect all candidates for reward model evaluation\n        mapping = []  # To map back reward model scores to corresponding nodes\n\n        # Collect candidates for reward model evaluation\n        for parent_node, parent_cum_logprob in current_level_nodes:\n            for candidate in batch_responses:\n                if parent_node + candidate.outputs[0].text + \"\\n\" not in [prm_input[1] for prm_input in prm_inputs]:\n                    new_node = parent_node + candidate.outputs[0].text + \"\\n\"\n                    cumulative_tokens = len(candidate.prompt_token_ids) + len(candidate.outputs[0].token_ids)\n                    prm_inputs.append((new_node[:-1] + \" \" + step_tag, new_node, parent_cum_logprob, cumulative_tokens))\n                    mapping.append(len(prm_inputs) - 1)  # Store the index for mapping back the score\n\n        # Batch reward model evaluation\n        prm_scores = eval_prm([prm_input for prm_input, _, _, _ in prm_inputs])\n        next_level_nodes = []\n\n        # Distribute candidates back to their parent nodes\n        for idx, (_, node, parent_cum_logprob, cumulative_tokens) in enumerate(prm_inputs):\n            score = prm_scores[idx]  # Get the corresponding score\n            new_cum_logprob = parent_cum_logprob + score  # Update cumulative log probability\n\n            # Check for completions and sufficient score\n            if \"```\" in node or \"\\\\boxed\" in node.split(\"\\n\\n\")[-1]:\n                completed_paths.append((node, new_cum_logprob))\n            elif score > math.log(0.8) and cumulative_tokens <= 2000:  # Threshold check\n                next_level_nodes.append((node, new_cum_logprob))\n\n        # Prune to keep only the top 'n' candidates based on cumulative log probability\n        next_level_nodes.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their cumulative log probability\n        current_level_nodes = next_level_nodes[:n]  # Keep only the top 'n' nodes\n        current_level += 1\n\n        # If we already have 'n' completed paths, no need to continue\n        if len(completed_paths) >= n:\n            break\n        if not current_level_nodes or current_level > 20:\n            if not completed_paths:\n                messages = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": problem + tool_instruction\n                    }\n                ]\n\n                base_prompt = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False\n                ) + \"```python\\n\"\n\n                raw_outputs = llm.generate([base_prompt]*8, tool_sampling_params)\n\n                for response in raw_outputs:\n                    completed_paths.append(\"```python\\n\" + response.outputs[0].text)\n            break\n\n    results = []\n    answers = []\n\n    for path in completed_paths:\n        try:\n            if \"```python\\n\" in path:\n                result_output, code_output = process_output(path)\n                gc.collect()\n                \n                results.append(code_output)\n                answers.append(code_output)\n            else:\n                raw_output = path[0].split(\"\\n\\n\")[-1]\n                result_output, code_output = process_output(raw_output)\n                gc.collect()\n\n                results.append(result_output)\n                answers.append(result_output)\n\n        except Exception as e:\n            print(e)\n            result_output, code_output = -1, -1\n            logprob = -10000\n\n            results.append(result_output)\n            answers.append(result_output)\n\n    total_results.append(results)\n    total_answers.append(answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:27:24.181514Z","iopub.execute_input":"2026-01-24T16:27:24.182416Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0959863d4e3147568971e8eb179caf2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c96e473afb4a0d81741378469c275f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40942cfc91634bccb68bd425f4d256ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec50b583a1ea4b9c80af50ce3b6e2ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45b07bbd9db4d0097d212d8c05e3a91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82eeae1913f40e8a46664d691e9200e"}},"metadata":{}},{"name":"stdout","text":"(k - sqrt(k*(k - l + 4)))**2/k**2 + (k + sqrt(k*(k - l + 4)))**2/k**2\n\nCODE RESULTS -1\nBOXED []\nBOXED 2\n2*(18*k - l + 4)/k\n\nCODE RESULTS -1\nBOXED []\nBOXED 2\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/kaggle/working/code.py\", line 33, in <module>\n    result = sum_of_squares()\n             ^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/code.py\", line 16, in sum_of_squares\n    distance = abs(solutions[0][0] - solutions[1][0])\n                                     ~~~~~~~~~^^^\nIndexError: list index out of range\n","output_type":"stream"},{"name":"stdout","text":"CODE RESULTS -1\nBOXED []\nBOXED 2\n(2*k - l + 4)/k\n\nCODE RESULTS -1\nBOXED []\nBOXED 2\n(-2*k + l - 2*sqrt(k*(k - l + 4)) + (k + sqrt(k*(k - l + 4)))**2/k)**2 + (-2*k + l + 2*sqrt(k*(k - l + 4)) + (k - sqrt(k*(k - l + 4)))**2/k)**2 + (k - sqrt(k*(k - l + 4)))**2/k**2 + (k + sqrt(k*(k - l + 4)))**2/k**2\n\nCODE RESULTS -1\nBOXED []\nBOXED 2\n2*(18*k - l + 4)/k\n\nCODE RESULTS -1\nBOXED []\nBOXED 16\n4*(29*k - l + 4)/(3*k)\n\nCODE RESULTS -1\nBOXED []\nBOXED 32\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/kaggle/working/code.py\", line 33, in <module>\n    result = sum_of_squares_distances()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/code.py\", line 16, in sum_of_squares_distances\n    x_values = [solution[x] for solution in solutions]\n                ~~~~~~~~^^^\nTypeError: tuple indices must be integers or slices, not Symbol\n 10%|█         | 1/10 [01:12<10:55, 72.83s/it]","output_type":"stream"},{"name":"stdout","text":"CODE RESULTS -1\nBOXED []\nBOXED 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"371770ffabb84873b2a43a20edff65d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82bc42657d214f9ea1522d8c04ee6759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb9d16de2c04e3387d093ff0e0cd406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6a7cb92575409483b1c2f58fb7dd83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4224a4412ff6414288f181e1982a6d32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b658775e814da9a7cf52155f82c049"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5208f1a5dc44ee3b56c5040aba6c6de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6540c16f55747dcab17dd75e84f9037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07238019cf84f48b6bb93161530c0bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa052ec22db46f2b62eee8629693ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233832ada20042b5ad0d84fc52484f9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa8a10a4bff4391b58a6f040ced738c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca881946d3b49e99dcbe80d6d41f793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecaad0be825944868d59d1fdb868e82b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d179013c1cb49c3971e2aa8505fbd7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29bcc9cb470e485aafbb1429e3cd60b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399faad0098c4f42b3d9a077340d9066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4059848562a644f0b424daefacf47722"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"### Main Loop","metadata":{}},{"cell_type":"markdown","source":"### Obtain Final Answers","metadata":{}},{"cell_type":"code","source":"print(f\"DataFrame rows: {len(df)}\")\nprint(f\"Total answer sets: {len(total_answers)}\")\nprint(f\"Total result sets: {len(total_results)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\nfinal_answers = []\n\nfor a, b in zip(total_answers, total_results):\n    a = np.array(a)\n    b = np.array(b)\n    print(\"a:\", a)\n    a[a < 0] = b[a < 0]\n\n    pred = Counter(a.tolist()).most_common(2)\n    print(\"Pred:\", pred)\n    try:\n        ans = pred[0][0] if not pred[0][0] < 0 else pred[1][0]\n    except:\n        if len(a) == 1:\n            ans = a[0]\n        else:\n            ans = 0\n\n    final_answers.append(ans)\n    print(ans)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['answer'] = final_answers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:25:03.211364Z","iopub.status.idle":"2026-01-24T16:25:03.211714Z","shell.execute_reply.started":"2026-01-24T16:25:03.211525Z","shell.execute_reply":"2026-01-24T16:25:03.211548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[['id','answer']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:25:03.213007Z","iopub.status.idle":"2026-01-24T16:25:03.213257Z","shell.execute_reply.started":"2026-01-24T16:25:03.213144Z","shell.execute_reply":"2026-01-24T16:25:03.21316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}